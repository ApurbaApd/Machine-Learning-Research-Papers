# BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
Author: Jacob Devlin, Ming-Wei Chang

Conference: arXiv

Link: https://arxiv.org/abs/1810.04805

Status: Pending

Topic: Text , Transformers

Year: 2018

# Questions

### What did authors try to accomplish?

### What were the key elements of the approach?

### What can you use yourself from this paper?

### What other references to follow?

---